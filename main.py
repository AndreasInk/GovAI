# review_app.py
"""
⚖️  HOA Drift-Checker – Streamlit UI
-----------------------------------
Flags low-similarity summary sentences, lets the board edit them,
and commits accepted edits back to GitHub as a pull request.

Quick-start
-----------
1.  pip install streamlit PyGithub streamlit-diff-viewer openai tiktoken
2.  export OPENAI_API_KEY=…
    export GITHUB_TOKEN=ghp_xxx
    export GITHUB_REPO=username/bylaws-rewrite   #   org/repo
3.  Have three companion files in the same folder:
    • chunks.json        – list[str]   chunk_id → source text
    • chunk_vecs.npy     – NumPy array of embeddings (same order)
    • flags.json         – list[tuple(similarity, sentence, [ids], reasoning)]
"""
from __future__ import annotations

import os
import json
import time
import re
from pathlib import Path
from typing import List, Tuple, Dict, Optional

from scipy.spatial.distance import cosine

import numpy as np
import streamlit as st
from st_diff_viewer import diff_viewer
from github import Github, InputGitAuthor

import ai  # your helper wrapper
from tiktoken import get_encoding           # just for token count display

# PDF generation
from fpdf import FPDF

# ------------------- Custom PDF class with branded header and footer -------------------
class PDFReport(FPDF):
    """Custom PDF class with branded header and footer."""
    def header(self):
        self.set_font("Helvetica", "B", 16)
        self.cell(0, 10, "Plantation Governance Report", ln=1, align="C")
        self.set_draw_color(100, 100, 100)
        self.set_line_width(0.3)
        self.line(self.l_margin, self.y, self.w - self.r_margin, self.y)
        self.ln(4)

    def footer(self):
        self.set_y(-15)
        self.set_draw_color(200, 200, 200)
        self.set_line_width(0.1)
        self.line(self.l_margin, self.y, self.w - self.r_margin, self.y)
        self.set_font("Helvetica", "I", 8)
        self.cell(0, 10, f"Page {self.page_no()}", align="C")
import io
import os
import unicodedata

# ------------------------------------------------------------------
# ⬇️  Load pre-computed artefacts  (produced by the notebook prototype)
# ------------------------------------------------------------------
DATA_DIR = Path(__file__).parent / "data"
chunks: List[str] = json.loads((DATA_DIR / "chunks.json").read_text())
embeddings = np.load(DATA_DIR / "chunk_vecs.npy")
flags: List[Tuple[float, str, List[int], str]] = json.loads((DATA_DIR / "flags.json").read_text())
# Store flags in session_state, sorted, for possible re-flagging
if "flags" not in st.session_state:
    flags.sort(key=lambda tup: tup[0])
    st.session_state.flags = flags

# Mapping from chunk_id (str) -> integer index in chunks list
ID2IDX_PATH = DATA_DIR / "id_to_idx.json"
id_to_idx: dict[str, int] = {}
if ID2IDX_PATH.exists():
    id_to_idx = json.loads(ID2IDX_PATH.read_text())
    # normalise keys to lower-case for robustness
    id_to_idx = {k.lower(): v for k, v in id_to_idx.items()}

def _cid_to_idx(cid: str | int) -> int | None:
    """Return integer index for a chunk ID (str or int)."""
    if isinstance(cid, int):
        return cid if 0 <= cid < len(chunks) else None
    return id_to_idx.get(str(cid).lower())

ENC = get_encoding("cl100k_base")

# ---------------------------------------------
# 🔐  GitHub client  (lazy-init on first commit)
# ---------------------------------------------
def _gh_client() -> Github:
    token = os.getenv("GITHUB_TOKEN")
    if not token:
        st.error("GITHUB_TOKEN env var missing – cannot push PRs.")
        st.stop()
    return Github(token)


# ----------------------------------------------------------
# 🛠️  Utility – open a PR with the edited draft.md content
# ----------------------------------------------------------
def create_or_update_pr(new_content: str, user_name: str = "HOA Reviewer") -> None:
    repo_full = os.getenv("GITHUB_REPO")
    if not repo_full:
        st.error("Set GITHUB_REPO (e.g. 'user/repo') to enable PRs.")
        return

    gh = _gh_client()
    repo = gh.get_repo(repo_full)

    base = repo.get_branch("main")
    branch_name = f"hoa-drift-fix/{int(time.time())}"
    repo.create_git_ref(ref=f"refs/heads/{branch_name}", sha=base.commit.sha)

    commit_message = "HOA summary edits via Streamlit reviewer"
    author = InputGitAuthor(user_name, f"{user_name.replace(' ','.').lower()}@example.com")
    repo.update_file(
        path="draft.md",
        message=commit_message,
        content=new_content,
        sha=repo.get_contents("draft.md", ref=branch_name).sha,
        branch=branch_name,
        author=author,
    )
    pr = repo.create_pull(
        title="🏷️ HOA drift fixes",
        body="Auto-generated by Streamlit reviewer; please squash-merge.",
        head=branch_name,
        base="main",
    )
    st.success(f"✅ Pull Request created: {pr.html_url}")


# -------------------------
# 🌐  Streamlit page config
# -------------------------
st.set_page_config(page_title="HOA Drift Checker", page_icon="📜", layout="wide")
st.title("📜 HOA Document Drift Checker")

st.sidebar.markdown(f"**{len(st.session_state.flags)} flags** loaded · Source chunks: **{len(chunks)}**")

# Global "edited draft" buffer (one long string)
if "draft_buffer" not in st.session_state:
    st.session_state.draft_buffer = Path("draft.md").read_text()

# --------------------------------------------------------------------
# 📚  Chunk Browser Functions
# --------------------------------------------------------------------
def parse_chunk_id(chunk_id: str) -> Dict[str, str]:
    """Parse chunk ID to extract document, page, and chunk number."""
    # Format: "document_page_chunk" (e.g., "bylaws_5_3")
    parts = chunk_id.split('_')
    if len(parts) >= 3:
        # Reconstruct document name (might contain underscores)
        doc_name = '_'.join(parts[:-2])
        page_num = parts[-2]
        chunk_num = parts[-1]
        return {
            'document': doc_name,
            'page': page_num,
            'chunk': chunk_num,
            'full_id': chunk_id
        }
    return {
        'document': 'unknown',
        'page': '0',
        'chunk': '0',
        'full_id': chunk_id
    }

def get_document_list() -> List[str]:
    """Get list of unique document names from chunk IDs."""
    documents = set()
    for chunk_id in id_to_idx.keys():
        doc_info = parse_chunk_id(chunk_id)
        documents.add(doc_info['document'])
    return sorted(list(documents))

def search_chunks(query: str, limit: int = 50) -> List[Tuple[int, str, float]]:
    """Search chunks using semantic similarity."""
    if not query.strip():
        return []
    
    query_vec = ai.embed(query)
    similarities = []
    
    for i, chunk in enumerate(chunks):
        chunk_vec = embeddings[i]
        sim = 1 - cosine(query_vec, chunk_vec)
        similarities.append((i, chunk, sim))
    
    # Sort by similarity and return top results
    similarities.sort(key=lambda x: x[2], reverse=True)
    return similarities[:limit]

def filter_chunks_by_document(document: str) -> List[Tuple[int, str, Dict[str, str]]]:
    """Filter chunks by document name."""
    filtered = []
    for chunk_id, idx in id_to_idx.items():
        doc_info = parse_chunk_id(chunk_id)
        if doc_info['document'] == document:
            filtered.append((idx, chunks[idx], doc_info))
    
    # Sort by page, then by chunk number
    filtered.sort(key=lambda x: (int(x[2]['page']), int(x[2]['chunk'])))
    return filtered

def filter_chunks_by_page(document: str, page: str) -> List[Tuple[int, str, Dict[str, str]]]:
    """Filter chunks by document and page."""
    filtered = []
    for chunk_id, idx in id_to_idx.items():
        doc_info = parse_chunk_id(chunk_id)
        if doc_info['document'] == document and doc_info['page'] == page:
            filtered.append((idx, chunks[idx], doc_info))
    
    # Sort by chunk number
    filtered.sort(key=lambda x: int(x[2]['chunk']))
    return filtered

# --------------------------------------------------------------------
# ---------- Re-flagging helper (uses cached chunk embeddings) ----------
# Accept IDs like `[C-123]`, `[C-Bylaws_5_3]`, or `[Bylaws_5_3]`
_CIT_RE = re.compile(r"(?:C-)?([\w\-]+)")

def _make_flags(draft_text: str, threshold: float = 0.85):
    """Return fresh flags list from *draft_text* (markdown)."""
    sentences = re.split(r"(?<=[.!?])\s+", draft_text)
    new_flags = []
    for s in sentences:
        src_ids = [x.lower() for x in _CIT_RE.findall(s)]
        if not src_ids:
            continue
        s_vec = ai.embed(s)  # (D,)
        idxs = [_cid_to_idx(cid) for cid in src_ids if _cid_to_idx(cid) is not None]
        if not idxs:
            continue  # no valid matching chunks
        worst = min(1 - cosine(s_vec, embeddings[i]) for i in idxs)
        if worst < threshold:
            new_flags.append((round(float(worst), 4), s, src_ids, "Vector similarity below threshold"))
    new_flags.sort(key=lambda tup: tup[0])
    return new_flags

def _to_latin1(text):
    """Replace common Unicode punctuation with ASCII equivalents and remove other non-latin-1 chars."""
    if not isinstance(text, str):
        return text
    # Replace curly quotes, dashes, ellipsis, etc.
    replacements = {
        '\u2018': "'", '\u2019': "'", '\u201c': '"', '\u201d': '"',
        '\u2013': '-', '\u2014': '-', '\u2026': '...', '\u2012': '-',
        '\u2010': '-', '\u2011': '-', '\u00a0': ' ',
    }
    for uni, ascii_ in replacements.items():
        text = text.replace(uni, ascii_)
    # Remove any remaining non-latin-1 chars
    return unicodedata.normalize('NFKD', text).encode('latin-1', 'ignore').decode('latin-1')

# --------------------------------------------------------------------
# 🎛️  Main Navigation
# --------------------------------------------------------------------
# Sidebar navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Choose a page:",
    ["Review Flags", "Browse All Chunks", "Search Chunks"],
    index=0
)

# --------------------------------------------------------------------
# 🚧  Review Flags Page
# --------------------------------------------------------------------
if page == "Review Flags":
    st.header("🚦 Review Drift Flags")
    
    # ---------- Optional re-flagging ----------
    uploaded = st.sidebar.file_uploader("Upload a *new* draft (Markdown)", type=["md", "txt"])
    if uploaded and st.sidebar.button("🔄 Re‑run flagging"):
        draft_bytes = uploaded.read()
        draft_text = draft_bytes.decode("utf-8", errors="ignore")
        with st.spinner("Computing new flags …"):
            st.session_state.flags = _make_flags(draft_text)
            st.success(f"✅ Re‑flagged draft – {len(st.session_state.flags)} flags found.")

    for idx, flag_data in enumerate(st.session_state.flags, 1):
        # Handle both old format (3-tuple) and new format (4-tuple with reasoning)
        if len(flag_data) == 3:
            sim, sent, ids = flag_data
            reasoning = "No reasoning provided"
        else:
            sim, sent, ids, reasoning = flag_data
            
        with st.expander(f"({idx}/{len(st.session_state.flags)}) Similarity {sim:.2f}  |  {sent[:80]}…"):
            col1, col2 = st.columns([1, 1])

            with col1:
                st.markdown("##### ✏️ **Edit summary sentence**")
                edited = st.text_area(
                    "Sentence", value=sent, key=f"edit-{idx}", height=80, label_visibility="collapsed"
                )
                token_len = len(ENC.encode(edited))
                st.caption(f"{token_len} tokens")
                
                # Display LLM reasoning if available
                if reasoning and reasoning != "No reasoning provided":
                    st.markdown("##### 🤖 **LLM Reasoning**")
                    st.info(reasoning)

            with col2:
                st.markdown("##### 📖 **Source chunk(s)**")
                if len(ids) == 1 and isinstance(ids[0], str) and ids[0] not in id_to_idx:
                    # This is likely a direct source text (from JSON input)
                    st.text_area("Source Text", value=ids[0], height=200, label_visibility="collapsed", disabled=True)
                elif not ids:
                    st.warning("⚠️ No source chunk(s) found for this flag.")
                else:
                    # This is citation-based input
                    for cid in ids:
                        idx2 = _cid_to_idx(cid)
                        if idx2 is not None:
                            st.write(chunks[idx2])
                            st.divider()
                        else:
                            st.warning(f"⚠️ Source chunk '{cid}' not found.")

            # 🖍️  Diff viewer (only show if edited != original)
            if edited.strip() != sent.strip():
                st.markdown("##### 🔍 Diff")
                diff_viewer(sent, edited, lang="md")

            # Keep a local mapping to inject back into full draft later
            if "edits" not in st.session_state:
                st.session_state.edits = {}
            st.session_state.edits[sent] = edited

    # --------------------------------------------------------------------
    # 💾  Commit all edits – rewrites draft.md and opens a PR
    # --------------------------------------------------------------------
    st.sidebar.divider()
    if st.sidebar.button("🚀 Commit **all** accepted edits → GitHub PR"):
        # Apply edits to the draft buffer
        new_draft = st.session_state.draft_buffer
        for original, replacement in st.session_state.edits.items():
            if original != replacement:
                new_draft = new_draft.replace(original, replacement)

        create_or_update_pr(new_draft, user_name=st.sidebar.text_input("Your name", value="HOA Reviewer"))

# --------------------------------------------------------------------
# 📚  Browse All Chunks Page
# --------------------------------------------------------------------
elif page == "Browse All Chunks":
    st.header("📚 Browse All Source Chunks")
    
    # Get list of documents
    documents = get_document_list()
    
    # Document selection
    selected_doc = st.selectbox(
        "Select Document:",
        ["All Documents"] + documents,
        index=0
    )
    
    if selected_doc == "All Documents":
        # Show all chunks with pagination
        st.subheader(f"All Chunks ({len(chunks)} total)")
        
        # Pagination
        chunks_per_page = 5  # Reduced for better side-by-side viewing
        total_pages = (len(chunks) + chunks_per_page - 1) // chunks_per_page
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            page_num = st.selectbox("Page:", range(1, total_pages + 1), index=0)
        
        start_idx = (page_num - 1) * chunks_per_page
        end_idx = min(start_idx + chunks_per_page, len(chunks))
        
        st.write(f"Showing chunks {start_idx + 1}-{end_idx} of {len(chunks)}")
        
        for i in range(start_idx, end_idx):
            chunk_id = None
            for cid, idx in id_to_idx.items():
                if idx == i:
                    chunk_id = cid
                    break
            
            if chunk_id:
                doc_info = parse_chunk_id(chunk_id)
                with st.expander(f"Chunk {i+1}: {doc_info['document']} (Page {doc_info['page']}, Chunk {doc_info['chunk']})"):
                    col1, col2 = st.columns([1, 1])
                    
                    with col1:
                        st.markdown("##### 📄 **Chunk Content**")
                        st.text_area(
                            "Chunk Text",
                            value=chunks[i],
                            height=200,
                            label_visibility="collapsed",
                            disabled=True,
                            key=f"chunk-content-{i}"
                        )
                        st.caption(f"ID: {chunk_id} | Tokens: {len(ENC.encode(chunks[i]))}")
                    
                    with col2:
                        st.markdown("##### 📖 **Source Context**")
                        # Show the original document info and any additional context
                        st.info(f"**Document:** {doc_info['document']}\n\n**Page:** {doc_info['page']}\n\n**Chunk:** {doc_info['chunk']}")
                        
                        # Show adjacent chunks for context (if available)
                        adjacent_chunks = []
                        for adj_idx in range(max(0, i-1), min(len(chunks), i+2)):
                            if adj_idx != i:
                                adj_chunk_id = None
                                for cid, idx in id_to_idx.items():
                                    if idx == adj_idx:
                                        adj_chunk_id = cid
                                        break
                                if adj_chunk_id:
                                    adj_doc_info = parse_chunk_id(adj_chunk_id)
                                    if adj_doc_info['document'] == doc_info['document'] and adj_doc_info['page'] == doc_info['page']:
                                        adjacent_chunks.append((adj_idx, chunks[adj_idx], adj_doc_info))
                        
                        if adjacent_chunks:
                            st.markdown("**Adjacent chunks on same page:**")
                            for adj_idx, adj_text, adj_info in adjacent_chunks:
                                with st.expander(f"Chunk {adj_info['chunk']} (ID: {adj_idx+1})"):
                                    st.text_area(
                                        "Adjacent Content",
                                        value=adj_text,
                                        height=100,
                                        label_visibility="collapsed",
                                        disabled=True,
                                        key=f"adjacent-chunk-{i}-{adj_idx}"
                                    )
                        else:
                            st.info("No adjacent chunks on the same page.")
    else:
        # Show chunks for specific document
        filtered_chunks = filter_chunks_by_document(selected_doc)
        st.subheader(f"{selected_doc} ({len(filtered_chunks)} chunks)")
        
        # Page selection for this document
        pages = sorted(list(set(chunk_info['page'] for _, _, chunk_info in filtered_chunks)))
        selected_page = st.selectbox(
            "Select Page:",
            ["All Pages"] + pages,
            index=0
        )
        
        if selected_page == "All Pages":
            display_chunks = filtered_chunks
        else:
            display_chunks = filter_chunks_by_page(selected_doc, selected_page)
        
        st.write(f"Showing {len(display_chunks)} chunks")
        
        for idx, chunk_text, chunk_info in display_chunks:
            with st.expander(f"Page {chunk_info['page']}, Chunk {chunk_info['chunk']}"):
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.markdown("##### 📄 **Chunk Content**")
                    st.text_area(
                        "Chunk Text",
                        value=chunk_text,
                        height=200,
                        label_visibility="collapsed",
                        disabled=True,
                        key=f"doc-chunk-content-{chunk_info['document']}-{chunk_info['page']}-{chunk_info['chunk']}"
                    )
                    st.caption(f"ID: {chunk_info['full_id']} | Tokens: {len(ENC.encode(chunk_text))}")
                
                with col2:
                    st.markdown("##### 📖 **Source Context**")
                    # Show document and page information
                    st.info(f"**Document:** {chunk_info['document']}\n\n**Page:** {chunk_info['page']}\n\n**Chunk:** {chunk_info['chunk']}")
                    
                    # Show adjacent chunks on the same page for context
                    adjacent_chunks = []
                    for adj_idx, adj_text, adj_info in display_chunks:
                        if (adj_info['page'] == chunk_info['page'] and 
                            adj_info['chunk'] != chunk_info['chunk']):
                            adjacent_chunks.append((adj_idx, adj_text, adj_info))
                    
                    if adjacent_chunks:
                        st.markdown("**Other chunks on this page:**")
                        for adj_idx, adj_text, adj_info in adjacent_chunks:
                            with st.expander(f"Chunk {adj_info['chunk']}"):
                                st.text_area(
                                    "Adjacent Content",
                                    value=adj_text,
                                    height=100,
                                    label_visibility="collapsed",
                                    disabled=True,
                                    key=f"doc-adjacent-chunk-{chunk_info['document']}-{chunk_info['page']}-{adj_info['chunk']}"
                                )
                    else:
                        st.info("This is the only chunk on this page.")

# --------------------------------------------------------------------
# 🔍  Search Chunks Page
# --------------------------------------------------------------------
elif page == "Search Chunks":
    st.header("🔍 Search Chunks")
    
    # Search interface
    search_query = st.text_input(
        "Enter search query:",
        placeholder="e.g., parking violations, assessment fees, board meetings..."
    )
    
    search_limit = st.slider("Number of results:", min_value=5, max_value=100, value=20)
    
    if search_query:
        with st.spinner("Searching chunks..."):
            results = search_chunks(search_query, limit=search_limit)
        
        if results:
            st.subheader(f"Search Results ({len(results)} found)")
            
            for i, (chunk_idx, chunk_text, similarity) in enumerate(results, 1):
                # Get chunk ID
                chunk_id = None
                for cid, idx in id_to_idx.items():
                    if idx == chunk_idx:
                        chunk_id = cid
                        break
                
                if chunk_id:
                    doc_info = parse_chunk_id(chunk_id)
                    with st.expander(f"{i}. Similarity: {similarity:.3f} | {doc_info['document']} (Page {doc_info['page']}, Chunk {doc_info['chunk']})"):
                        st.text_area(
                            "Content",
                            value=chunk_text,
                            height=150,
                            label_visibility="collapsed",
                            disabled=True,
                            key=f"search-result-{i}-{chunk_idx}"
                        )
                        st.caption(f"ID: {chunk_id} | Tokens: {len(ENC.encode(chunk_text))}")
        else:
            st.info("No results found. Try a different search term.")
    else:
        st.info("Enter a search query to find relevant chunks.")

# ------------------------------------------------------------------
# 📥  Download JSON Draft as PDF
# ------------------------------------------------------------------
st.sidebar.divider()
st.sidebar.markdown("### Download JSON Draft as PDF")

# Try to load draft.json as default
default_draft_data = None
try:
    if Path("draft.json").exists():
        with open("draft.json", "r") as f:
            default_draft_data = json.load(f)
        st.sidebar.success("✅ Loaded draft.json as default")
except Exception as e:
    st.sidebar.warning(f"⚠️ Could not load draft.json: {e}")

json_draft_file = st.sidebar.file_uploader("Upload JSON draft", type=["json"], key="json-draft-upload")

# Use default data if no file is uploaded
if json_draft_file is not None:
    try:
        draft_data = json.load(json_draft_file)
    except Exception as e:
        st.sidebar.error(f"Failed to parse uploaded file: {e}")
        draft_data = None
elif default_draft_data is not None:
    draft_data = default_draft_data
    st.sidebar.info("📄 Using draft.json as default")
else:
    draft_data = None
if draft_data is not None:
    pairs = []
    executive_summary = None
    # Check for executive_summary and sections (GovAI draft format)
    if isinstance(draft_data, dict) and "sections" in draft_data:
        executive_summary = draft_data.get("executive_summary")
        for section in draft_data["sections"]:
            summary = section.get("summary_text", "")
            source = section.get("source_text", "")
            doc = section.get("source_document", "")
            page = section.get("source_page", "")
            pairs.append({
                "summary": summary,
                "source": source,
                "document": doc,
                "page": page
            })
    else:
        # Fallback to previous logic
        if isinstance(draft_data, list):
            for item in draft_data:
                if isinstance(item, dict) and "summary" in item and "source_text" in item:
                    pairs.append({"summary": item["summary"], "source": item["source_text"], "document": "", "page": ""})
                elif isinstance(item, (list, tuple)) and len(item) == 2:
                    pairs.append({"summary": item[0], "source": item[1], "document": "", "page": ""})
        elif isinstance(draft_data, dict) and "summary" in draft_data and "source_text" in draft_data:
            pairs.append({"summary": draft_data["summary"], "source": draft_data["source_text"], "document": "", "page": ""})
        else:
            st.sidebar.warning("Could not parse summary/source pairs from JSON.")
            pairs = []
    if pairs or executive_summary:
        try:
            # Generate PDF in memory using custom branded PDF class
            pdf = PDFReport()
            pdf.set_auto_page_break(auto=True, margin=15)

            from datetime import datetime

            # Cover page
            pdf.add_page()
            pdf.set_font("Helvetica", "B", 28)
            pdf.ln(45)  # vertical spacing
            pdf.multi_cell(0, 14, _to_latin1("Plantation Governance Report"), align="C")
            pdf.set_font("Helvetica", "", 14)
            pdf.ln(8)
            pdf.multi_cell(0, 10, datetime.now().strftime("Generated on %B %d, %Y"), align="C")
            pdf.add_page()

            # Optional Executive Summary
            if executive_summary:
                pdf.set_font("Helvetica", "B", 18)
                pdf.multi_cell(0, 12, _to_latin1("Executive Summary"))
                pdf.ln(2)
                pdf.set_font("Helvetica", "", 12)
                pdf.multi_cell(0, 8, _to_latin1(executive_summary))

            # Main Sections
            for idx, pair in enumerate(pairs, 1):
                # Automatic page break to avoid overflow
                if pdf.get_y() > pdf.h - pdf.b_margin - 20:
                    pdf.add_page()

                # Separator between sections
                if idx > 1:
                    pdf.set_draw_color(200, 200, 200)
                    pdf.set_line_width(0.2)
                    y = pdf.get_y()
                    pdf.line(pdf.l_margin, y, pdf.w - pdf.r_margin, y)
                    pdf.ln(4)

                # Section heading with context
                pdf.set_font("Helvetica", "B", 16)
                header_text = f"Section {idx}"
                if pair["document"] or pair["page"]:
                    header_text += f" – {pair['document']} (Page {pair['page']})"
                pdf.multi_cell(0, 12, _to_latin1(header_text))
                pdf.ln(4)

                # Document context
                if pair["document"] or pair["page"]:
                    pdf.set_font("Helvetica", "I", 11)
                    context_line = " · ".join(filter(None, [pair["document"], f"Page {pair['page']}"]))
                    pdf.multi_cell(0, 8, _to_latin1(context_line))
                    pdf.ln(1)

                # Summary
                pdf.set_font("Helvetica", "B", 13)
                pdf.multi_cell(0, 9, _to_latin1("Key Insights"))
                pdf.set_font("Helvetica", "", 12)
                pdf.multi_cell(0, 8, _to_latin1(pair["summary"]))
                pdf.ln(1)

                # Source
                pdf.set_font("Helvetica", "B", 13)
                pdf.multi_cell(0, 9, _to_latin1("Source Excerpt"))
                pdf.set_font("Helvetica", "I", 11)
                pdf.set_text_color(80, 80, 80)
                pdf.multi_cell(0, 8, _to_latin1(pair["source"]))
                pdf.set_text_color(0, 0, 0)
                pdf.ln(4)

            pdf_bytes = pdf.output(dest='S').encode('latin-1')
            pdf_buffer = io.BytesIO(pdf_bytes)
            st.sidebar.download_button(
                label="Download PDF",
                data=pdf_buffer,
                file_name="draft_summaries.pdf",
                mime="application/pdf"
            )
        except Exception as e:
            st.sidebar.error(f"Failed to generate PDF: {e}")
    else:
        st.sidebar.info("No summary/source pairs found in JSON.")